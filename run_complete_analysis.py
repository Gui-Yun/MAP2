# ç¥ç»æ•°æ®å®Œæ•´åˆ†æä¸»è„šæœ¬
# guiy24@mails.tsinghua.edu.cn
# ä¸€é”®è¿è¡Œæ‰€æœ‰åˆ†ææ¨¡å—ï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ

import os
import sys
import time
import traceback
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import matplotlib
matplotlib.use('Agg')
# å¯¼å…¥é…ç½®
from loaddata import cfg

def create_analysis_log():
    """åˆ›å»ºåˆ†ææ—¥å¿—è®°å½•"""
    log_dir = os.path.join(cfg.DATA_PATH if hasattr(cfg, 'DATA_PATH') else 'results', 'analysis_logs')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f'complete_analysis_{timestamp}.log')
    
    return log_file

def log_message(message, log_file=None, print_console=True):
    """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    
    if print_console:
        print(log_entry)
    
    if log_file:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry + '\n')

def run_analysis_module(module_name, description, log_file):
    """è¿è¡Œå•ä¸ªåˆ†ææ¨¡å—"""
    log_message(f"å¼€å§‹è¿è¡Œ: {description}", log_file)
    start_time = time.time()
    
    try:
        if module_name == 'loaddata':
            # åŸºç¡€æ•°æ®å¤„ç†å’ŒRRç¥ç»å…ƒç­›é€‰
            import subprocess
            import sys
            result = subprocess.run([sys.executable, 'loaddata.py'], 
                                  capture_output=True, text=True, cwd='.')
            if result.returncode == 0:
                log_message("âœ“ åŸºç¡€æ•°æ®å¤„ç†å®Œæˆ", log_file)
            else:
                log_message(f"âœ— åŸºç¡€æ•°æ®å¤„ç†å¤±è´¥: {result.stderr}", log_file)
                raise Exception(f"loaddata.py execution failed: {result.stderr}")
            
        elif module_name == 'network':
            # ç½‘ç»œæ‹“æ‰‘åˆ†æ
            import network
            log_message("âœ“ ç½‘ç»œæ‹“æ‰‘åˆ†æå®Œæˆ", log_file)
            
        elif module_name == 'advanced_network':
            # é«˜çº§ç½‘ç»œåˆ†æ
            from advanced_network_analysis import run_advanced_network_analysis
            run_advanced_network_analysis()
            log_message("âœ“ é«˜çº§ç½‘ç»œåˆ†æå®Œæˆ", log_file)
            
        elif module_name == 'noise_correlation':
            # å™ªå£°ç›¸å…³æ€§åˆ†æ
            from noise_correlation_analysis import run_noise_correlation_analysis
            run_noise_correlation_analysis()
            log_message("âœ“ å™ªå£°ç›¸å…³æ€§åˆ†æå®Œæˆ", log_file)
            
        elif module_name == 'degree_centrality':
            # åº¦ä¸­å¿ƒæ€§ä¸ç¥ç»ä¿¡æ¯å…³ç³»åˆ†æ
            import degree
            log_message("âœ“ åº¦ä¸­å¿ƒæ€§åˆ†æå®Œæˆ", log_file)
            
        elif module_name == 'manifold':
            # æµå½¢å­¦ä¹ åˆ†æ
            import manifold
            log_message("âœ“ æµå½¢å­¦ä¹ åˆ†æå®Œæˆ", log_file)
            
        
        elapsed = time.time() - start_time
        log_message(f"âœ“ {description} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}ç§’", log_file)
        return True
        
    except Exception as e:
        elapsed = time.time() - start_time
        log_message(f"âœ— {description} å¤±è´¥ï¼Œè€—æ—¶: {elapsed:.1f}ç§’", log_file)
        log_message(f"é”™è¯¯ä¿¡æ¯: {str(e)}", log_file)
        log_message(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}", log_file, print_console=False)
        return False

def collect_visualization_results():
    """æ”¶é›†æ‰€æœ‰å¯è§†åŒ–ç»“æœ"""
    visualizations = {}
    seen_files = set()  # ç”¨äºå»é‡
    
    # å®šä¹‰è¦æ”¶é›†çš„ç›®å½•å’Œæ–‡ä»¶æ¨¡å¼
    search_paths = [
        ('results/figures', 'åŸºç¡€åˆ†æå¯è§†åŒ–'),
        ('results/advanced_analysis', 'é«˜çº§ç½‘ç»œåˆ†æå¯è§†åŒ–'),
        ('results/noise_correlation', 'å™ªå£°ç›¸å…³åˆ†æå¯è§†åŒ–'),
        (os.path.join(getattr(cfg, 'DATA_PATH', ''), 'centrality_results') if hasattr(cfg, 'DATA_PATH') else None, 'ä¸­å¿ƒæ€§åˆ†æå¯è§†åŒ–'),
        (os.path.join(getattr(cfg, 'DATA_PATH', ''), 'manifold_results') if hasattr(cfg, 'DATA_PATH') else None, 't-SNEæµå½¢å­¦ä¹ å¯è§†åŒ–'),
        ('results/manifold_results', 't-SNEæµå½¢å­¦ä¹ å¯è§†åŒ–')  # æ·»åŠ æœ¬åœ°manifoldç»“æœç›®å½•
    ]
    
    for path, category in search_paths:
        if path and os.path.exists(path):
            visualizations[category] = []
            for root, dirs, files in os.walk(path):
                for file in files:
                    if file.endswith('.png'):
                        full_path = os.path.abspath(os.path.join(root, file))
                        # ä½¿ç”¨ç»å¯¹è·¯å¾„å»é‡
                        if full_path not in seen_files:
                            seen_files.add(full_path)
                            visualizations[category].append({
                                'filename': file,
                                'path': full_path,
                                'description': get_figure_description(file)
                            })
    
    return visualizations

def get_figure_description(filename):
    """æ ¹æ®æ–‡ä»¶åç”Ÿæˆå›¾è¡¨æè¿°"""
    descriptions = {
        # åŸºç¡€åˆ†æå¯è§†åŒ–
        'neural_activity_heatmap.png': 'ç¥ç»æ´»åŠ¨çƒ­å›¾ - æ˜¾ç¤ºç¥ç»å…ƒæ´»åŠ¨çš„æ—¶ç©ºæ¨¡å¼',
        'classification_performance.png': 'åˆ†ç±»æ€§èƒ½å¯¹æ¯” - å¤šç§åˆ†ç±»å™¨çš„å‡†ç¡®ç‡æ¯”è¾ƒ',
        'roc_curves.png': 'ROCæ›²çº¿åˆ†æ - å„åˆ†ç±»å™¨çš„æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿å¯¹æ¯”',
        'accuracy_over_time.png': 'æ—¶é—´åºåˆ—åˆ†ç±»å‡†ç¡®ç‡ - ä¸åŒæ—¶é—´ç‚¹çš„åˆ†ç±»æ€§èƒ½',
        'fisher_information.png': 'Fisherä¿¡æ¯åˆ†æ - ä¿¡æ¯ç¼–ç èƒ½åŠ›éšæ—¶é—´å˜åŒ–',
        'fisher_heatmap.png': 'Fisherä¿¡æ¯çƒ­å›¾ - ç¥ç»å…ƒåœ¨æ—¶é—´å’Œç±»åˆ«ä¸Šçš„ä¿¡æ¯ç¼–ç å¼ºåº¦',
        'combined_analysis.png': 'ç»¼åˆåˆ†æ - Fisherä¿¡æ¯ä¸åˆ†ç±»å‡†ç¡®ç‡çš„è”åˆåˆ†æ',
        'neuron_count_effect.png': 'ç¥ç»å…ƒæ•°é‡æ•ˆåº” - ç¥ç»å…ƒæ•°é‡å¯¹æ€§èƒ½çš„å½±å“',
        'rr_neurons_spatial.png': 'RRç¥ç»å…ƒç©ºé—´åˆ†å¸ƒ - å“åº”å¯é ç¥ç»å…ƒçš„ç©ºé—´ä½ç½®',
        'trigger_distribution.png': 'åˆºæ¿€è§¦å‘åˆ†å¸ƒ - åˆºæ¿€æ—¶é—´åºåˆ—çš„ç»Ÿè®¡ç‰¹æ€§',
        'stimulus_distribution.png': 'åˆºæ¿€æ•°æ®åˆ†å¸ƒ - åˆºæ¿€ç±»åˆ«å’Œå¼ºåº¦çš„åˆ†å¸ƒ',
        'network_topology.png': 'ç½‘ç»œæ‹“æ‰‘ç»“æ„ - ç¥ç»å…ƒè¿æ¥ç½‘ç»œçš„æ•´ä½“ç»“æ„',
        
        # ä¸­å¿ƒæ€§åˆ†æ
        'degree_centrality_vs_information.png': 'åº¦ä¸­å¿ƒæ€§ä¸ä¿¡æ¯å…³ç³» - ç½‘ç»œä¸­å¿ƒæ€§ä¸ä¿¡æ¯ç¼–ç çš„å…³ç³»',
        'centrality_distributions.png': 'ä¸­å¿ƒæ€§æŒ‡æ ‡åˆ†å¸ƒ - å¤šç§ä¸­å¿ƒæ€§æŒ‡æ ‡çš„ç»Ÿè®¡åˆ†å¸ƒ',
        'centrality_relationships.png': 'ä¸­å¿ƒæ€§æŒ‡æ ‡ç›¸å…³æ€§ - ä¸åŒä¸­å¿ƒæ€§æŒ‡æ ‡é—´çš„å…³ç³»',
        'multivariate_fisher_regression.png': 'å¤šå˜é‡Fisherä¿¡æ¯å›å½’ - å±‚çº§ä¸­å¿ƒæ€§ä¸é›†åˆä¿¡æ¯ç¼–ç å…³ç³»',
        
        # é«˜çº§ç½‘ç»œåˆ†æ
        'rich_club_condition_1.png': 'å¯Œäººä¿±ä¹éƒ¨åˆ†æ(æ¡ä»¶1) - æ¡ä»¶1ä¸‹é«˜åº¦è¿æ¥èŠ‚ç‚¹çš„é›†ç¾¤ç‰¹æ€§',
        'rich_club_condition_2.png': 'å¯Œäººä¿±ä¹éƒ¨åˆ†æ(æ¡ä»¶2) - æ¡ä»¶2ä¸‹é«˜åº¦è¿æ¥èŠ‚ç‚¹çš„é›†ç¾¤ç‰¹æ€§', 
        'rich_club_condition_3.png': 'å¯Œäººä¿±ä¹éƒ¨åˆ†æ(æ¡ä»¶3) - æ¡ä»¶3ä¸‹é«˜åº¦è¿æ¥èŠ‚ç‚¹çš„é›†ç¾¤ç‰¹æ€§',
        'pid_condition_multi_condition_breakdown.png': 'PIDå¤šæ¡ä»¶åˆ†è§£åˆ†æ - ä¿¡æ¯åˆ†è§£çš„æ¡ä»¶ä¾èµ–æ€§',
        'pid_condition_multi_condition_components.png': 'PIDå¤šæ¡ä»¶æˆåˆ†åˆ†æ - å„ä¿¡æ¯æˆåˆ†çš„æ¡ä»¶æ¯”è¾ƒ',
        'small_world_analysis.png': 'å°ä¸–ç•Œç½‘ç»œåˆ†æ - ç½‘ç»œçš„å°ä¸–ç•Œç‰¹æ€§é‡åŒ–',
        
        # å™ªå£°ç›¸å…³åˆ†æ
        'correlation_matrices.png': 'ç›¸å…³æ€§çŸ©é˜µ - ç¥ç»å…ƒé—´ä¿¡å·å’Œå™ªå£°ç›¸å…³æ€§å¯¹æ¯”',
        'hub_peripheral_analysis.png': 'ä¸­å¿ƒ-å¤–å›´åˆ†æ - ç½‘ç»œä¸­å¿ƒèŠ‚ç‚¹ä¸å¤–å›´èŠ‚ç‚¹çš„ç›¸å…³æ€§ç‰¹å¾',
        'network_metrics_comparison.png': 'ç½‘ç»œæŒ‡æ ‡æ¯”è¾ƒ - ä¿¡å·ç½‘ç»œä¸å™ªå£°ç½‘ç»œçš„æ‹“æ‰‘æŒ‡æ ‡å¯¹æ¯”',
        'neuron_pairs_scatter.png': 'ç¥ç»å…ƒå¯¹æ•£ç‚¹å›¾ - ç¥ç»å…ƒå¯¹é—´ç›¸å…³æ€§çš„åˆ†å¸ƒæ¨¡å¼',
        'noise_signal_comparison.png': 'å™ªå£°-ä¿¡å·æ¯”è¾ƒ - å™ªå£°ç›¸å…³ä¸ä¿¡å·ç›¸å…³çš„å®šé‡æ¯”è¾ƒ',
        'shuffle_effects.png': 'éšæœºåŒ–æ•ˆåº” - æ•°æ®éšæœºåŒ–å¯¹ç›¸å…³æ€§åˆ†æçš„å½±å“',
        'noise_correlation_analysis.png': 'å™ªå£°ç›¸å…³åˆ†æ - ç¥ç»å…ƒé—´çš„å™ªå£°ç›¸å…³æ€§æ¨¡å¼',
        
        # t-SNEæµå½¢å­¦ä¹ åˆ†æ
        'tsne_scientific_analysis.png': 't-SNEç§‘ç ”åˆ†æ - ç¥ç»ç¾¤ä½“é™ç»´çš„ç»¼åˆç§‘ç ”é£æ ¼å¯è§†åŒ–',
        'tsne_publication_figure.png': 't-SNEè®ºæ–‡å›¾ - é€‚åˆå­¦æœ¯å‘è¡¨çš„ç¥ç»ç¾¤ä½“æµå½¢å¯è§†åŒ–',
        'manifold_visualization.png': 'æµå½¢å¯è§†åŒ– - é«˜ç»´ç¥ç»æ•°æ®çš„ä½ç»´åµŒå…¥',
        
        # ä¼ ç»Ÿé™ç»´æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        'pca_2d_analysis.png': 'PCAäºŒç»´åˆ†æ - ä¸»æˆåˆ†åˆ†æé™ç»´å¯è§†åŒ–',
        'pca_3d_analysis.png': 'PCAä¸‰ç»´åˆ†æ - ä¸»æˆåˆ†åˆ†æé™ç»´å¯è§†åŒ–'
    }
    
    # æ¨¡ç³ŠåŒ¹é…
    for key, desc in descriptions.items():
        if key.replace('.png', '') in filename.replace('.png', ''):
            return desc
    
    return 'åˆ†æç»“æœå¯è§†åŒ–'

def generate_markdown_report(log_file, results_summary, total_elapsed):
    """ç”Ÿæˆæ•´åˆçš„MarkdownæŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
    report_file = os.path.join(os.path.dirname(log_file), f'åˆ†ææŠ¥å‘Š_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md')
    report_dir = os.path.dirname(report_file)
    
    # æ”¶é›†å¯è§†åŒ–ç»“æœ
    visualizations = collect_visualization_results()
    
    # è¯»å–æ—¥å¿—æ–‡ä»¶ä¸­çš„å…³é”®ä¿¡æ¯
    important_logs = []
    if os.path.exists(log_file):
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                if any(keyword in line for keyword in ['âœ“', 'âœ—', 'ç›¸å…³æ€§åˆ†æ', 'Fisherä¿¡æ¯', 'å‡†ç¡®ç‡', 'ç¥ç»å…ƒæ•°', 'ç½‘ç»œå¯†åº¦']):
                    important_logs.append(line.strip())
    
    markdown_content = f"""# ç¥ç»æ•°æ®åˆ†æå®Œæ•´æŠ¥å‘Š

**åˆ†ææ—¶é—´**: {timestamp}  
**æ€»è€—æ—¶**: {total_elapsed:.1f}ç§’ ({total_elapsed/60:.1f}åˆ†é’Ÿ)  
**åˆ†æç‰ˆæœ¬**: {getattr(cfg, 'LOADER_VERSION', 'unknown')}  

## ğŸ“Š åˆ†æç»“æœæ¦‚è§ˆ

### âœ… æˆåŠŸå®Œæˆçš„åˆ†ææ¨¡å—
"""
    
    # æ·»åŠ æˆåŠŸæ¨¡å—
    successful = [item for item, success in results_summary.items() if success]
    failed = [item for item, success in results_summary.items() if not success]
    
    for item in successful:
        markdown_content += f"- âœ… **{item}**\n"
    
    if failed:
        markdown_content += "\n### âŒ å¤±è´¥çš„åˆ†ææ¨¡å—\n"
        for item in failed:
            markdown_content += f"- âŒ **{item}**\n"
    
    markdown_content += f"\n**æˆåŠŸç‡**: {len(successful)}/{len(results_summary)} ({len(successful)/len(results_summary)*100:.1f}%)\n"
    
    # æ·»åŠ é‡è¦æ—¥å¿—ä¿¡æ¯
    if important_logs:
        markdown_content += "\n## ğŸ“‹ é‡è¦åˆ†æç»“æœ\n\n```\n"
        for log in important_logs[-20:]:  # åªæ˜¾ç¤ºæœ€å20æ¡é‡è¦æ—¥å¿—
            markdown_content += log + "\n"
        markdown_content += "```\n"
    
    # æ·»åŠ å¯è§†åŒ–ç»“æœ
    markdown_content += "\n## ğŸ¨ å¯è§†åŒ–ç»“æœå±•ç¤º\n"
    
    if not visualizations:
        markdown_content += "\nâš ï¸ æœªæ‰¾åˆ°å¯è§†åŒ–ç»“æœæ–‡ä»¶\n"
    else:
        for category, figures in visualizations.items():
            if figures:
                markdown_content += f"\n### {category}\n"
                for fig in figures:
                    # ä½¿ç”¨æ ‡å‡†ç»å¯¹è·¯å¾„ï¼Œå…¼å®¹æ›´å¤šMarkdownæ¸²æŸ“å™¨
                    abs_fig_path = os.path.abspath(fig['path']).replace('\\', '/')
                    
                    markdown_content += f"\n#### {fig['filename']}\n"
                    markdown_content += f"{fig['description']}\n\n"
                    markdown_content += f"![{fig['filename']}]({abs_fig_path})\n\n"
                    markdown_content += f"**å›¾ç‰‡è·¯å¾„**: `{abs_fig_path}`\n\n"
                    markdown_content += "---\n"
    
    # æ·»åŠ æ•°æ®æ–‡ä»¶ä¿¡æ¯
    markdown_content += "\n## ğŸ“ ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶\n"
    
    base_dirs = ['results']
    if hasattr(cfg, 'DATA_PATH'):
        base_dirs.append(cfg.DATA_PATH)
    
    for base_dir in base_dirs:
        if os.path.exists(base_dir):
            markdown_content += f"\n### {base_dir}/ ç›®å½•\n"
            data_files = []
            for root, dirs, files in os.walk(base_dir):
                for file in files:
                    if file.endswith(('.npz', '.mat', '.csv')):
                        relative_path = os.path.relpath(os.path.join(root, file), base_dir)
                        data_files.append(relative_path)
            
            if data_files:
                for file in sorted(data_files):
                    file_desc = get_data_file_description(file)
                    markdown_content += f"- **{file}** - {file_desc}\n"
            else:
                markdown_content += "- æš‚æ— æ•°æ®æ–‡ä»¶\n"
    
    # æ·»åŠ åˆ†ææ–¹æ³•è¯´æ˜
    markdown_content += f"""

## ğŸ”¬ åˆ†ææ–¹æ³•è¯´æ˜

### æ•°æ®é¢„å¤„ç†
- **RRç¥ç»å…ƒç­›é€‰**: ä½¿ç”¨ç»Ÿè®¡æ£€éªŒè¯†åˆ«å“åº”å¯é çš„ç¥ç»å…ƒ
- **åŸºçº¿æ ¡æ­£**: dF/Fæ ‡å‡†åŒ–å¤„ç†
- **æ—¶é—´çª—å£**: åŸºçº¿æœŸ{getattr(cfg, 'PRE_FRAMES', 10)}å¸§ï¼Œåˆºæ¿€æœŸ{getattr(cfg, 'STIMULUS_DURATION', 20)}å¸§

### ç½‘ç»œåˆ†æ
- **ç½‘ç»œæ„å»º**: åŸºäºPearsonç›¸å…³ç³»æ•°æ„å»ºåŠŸèƒ½è¿æ¥ç½‘ç»œ
- **æ‹“æ‰‘æŒ‡æ ‡**: åº¦ä¸­å¿ƒæ€§ã€ä»‹æ•°ä¸­å¿ƒæ€§ã€æ¥è¿‘ä¸­å¿ƒæ€§ã€ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
- **é«˜çº§åˆ†æ**: å°ä¸–ç•Œç½‘ç»œç‰¹æ€§ã€å¯Œäººä¿±ä¹éƒ¨æ•ˆåº”

### ä¿¡æ¯åˆ†æ
- **å•ç¥ç»å…ƒFisherä¿¡æ¯**: ç±»é—´æ–¹å·®/ç±»å†…æ–¹å·®
- **å¤šå˜é‡Fisherä¿¡æ¯**: åŸºäºæ•£å¸ƒçŸ©é˜µçš„åˆ¤åˆ«åˆ†æ
- **åˆ†ç±»åˆ†æ**: å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§èƒ½æ¯”è¾ƒ

### å…¶ä»–åˆ†æ
- **å™ªå£°ç›¸å…³åˆ†æ**: ç¥ç»å…ƒé—´ä¿¡å·ç›¸å…³æ€§å’Œå™ªå£°ç›¸å…³æ€§
- **æµå½¢å­¦ä¹ **: t-SNEã€UMAPç­‰é™ç»´å¯è§†åŒ–æ–¹æ³•

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜è¯·è”ç³»ï¼šguiy24@mails.tsinghua.edu.cn

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}*  
*åˆ†æå·¥å…·ç‰ˆæœ¬: Neural Analysis Toolkit v1.0*
"""
    
    # ä¿å­˜MarkdownæŠ¥å‘Š
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    # åŒæ—¶ç”ŸæˆHTMLç‰ˆæœ¬ï¼Œå›¾ç‰‡é“¾æ¥æ›´å…¼å®¹
    html_file = report_file.replace('.md', '.html')
    generate_html_report(markdown_content, html_file, visualizations)
    
    return report_file

def generate_html_report(markdown_content, html_file, visualizations):
    """ç”ŸæˆHTMLç‰ˆæœ¬æŠ¥å‘Šï¼Œå›¾ç‰‡æ˜¾ç¤ºæ›´å…¼å®¹"""
    
    # å…ˆå¤„ç†å›¾ç‰‡é“¾æ¥ï¼Œä½¿ç”¨file://åè®®
    html_content = markdown_content
    for category, figures in visualizations.items():
        if figures:
            for fig in figures:
                abs_fig_path = os.path.abspath(fig['path']).replace('\\', '/')
                file_url = f"file:///{abs_fig_path}"
                
                # æ›¿æ¢å›¾ç‰‡æ ‡è®°ä¸ºHTML imgæ ‡ç­¾
                img_pattern = f"![{fig['filename']}]({abs_fig_path})"
                img_html = f'<img src="{file_url}" alt="{fig['filename']}" style="max-width:800px;height:auto;" />'
                html_content = html_content.replace(img_pattern, img_html)
    
    # ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢ï¼ˆæŒ‰é¡ºåºå¤„ç†ï¼Œé¿å…åµŒå¥—é—®é¢˜ï¼‰
    lines = html_content.split('\n')
    html_lines = []
    
    in_code_block = False
    
    for line in lines:
        # å¤„ç†ä»£ç å—
        if line.startswith('```'):
            if not in_code_block:
                html_lines.append('<pre><code>')
                in_code_block = True
            else:
                html_lines.append('</code></pre>')
                in_code_block = False
            continue
            
        if in_code_block:
            html_lines.append(line)
            continue
            
        # å¤„ç†æ ‡é¢˜
        if line.startswith('#### '):
            html_lines.append(f'<h4>{line[5:]}</h4>')
        elif line.startswith('### '):
            html_lines.append(f'<h3>{line[4:]}</h3>')
        elif line.startswith('## '):
            html_lines.append(f'<h2>{line[3:]}</h2>')
        elif line.startswith('# '):
            html_lines.append(f'<h1>{line[2:]}</h1>')
        # å¤„ç†ç²—ä½“
        elif '**' in line:
            line = line.replace('**', '<strong>', 1).replace('**', '</strong>', 1)
            html_lines.append(f'<p>{line}</p>' if line.strip() else '<br>')
        # å¤„ç†ä»£ç 
        elif line.startswith('**å›¾ç‰‡è·¯å¾„**: `') and line.endswith('`'):
            path_content = line[13:-1]  # æå–è·¯å¾„å†…å®¹
            html_lines.append(f'<p class="path"><strong>å›¾ç‰‡è·¯å¾„</strong>: <code>{path_content}</code></p>')
        # å¤„ç†åˆ†å‰²çº¿
        elif line.strip() == '---':
            html_lines.append('<hr>')
        # å¤„ç†åˆ—è¡¨é¡¹
        elif line.startswith('- '):
            html_lines.append(f'<li>{line[2:]}</li>')
        # å¤„ç†ç©ºè¡Œ
        elif line.strip() == '':
            html_lines.append('<br>')
        # å¤„ç†æ™®é€šæ®µè½
        else:
            if line.strip():
                html_lines.append(f'<p>{line}</p>')
    
    html_content = '\n'.join(html_lines)
    
    # æ·»åŠ HTMLæ¡†æ¶
    full_html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¥ç»æ•°æ®åˆ†æå®Œæ•´æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', sans-serif; line-height: 1.6; margin: 40px; }}
        h1, h2, h3, h4 {{ color: #333; }}
        img {{ display: block; margin: 20px 0; border: 1px solid #ddd; border-radius: 8px; }}
        code {{ background: #f4f4f4; padding: 2px 4px; border-radius: 3px; }}
        pre {{ background: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        hr {{ border: none; border-top: 1px solid #ddd; margin: 30px 0; }}
        .path {{ font-family: monospace; font-size: 0.9em; color: #666; }}
    </style>
</head>
<body>
<p>{html_content}</p>
</body>
</html>"""
    
    # ä¿å­˜HTMLæ–‡ä»¶
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(full_html)

def get_data_file_description(filename):
    """æ ¹æ®æ–‡ä»¶åç”Ÿæˆæ•°æ®æ–‡ä»¶æè¿°"""
    descriptions = {
        'classification_results.npz': 'åˆ†ç±»ç»“æœæ•°æ®',
        'accuracy_over_time.npz': 'æ—¶é—´åºåˆ—å‡†ç¡®ç‡æ•°æ®',
        'fisher_over_time.npz': 'Fisherä¿¡æ¯æ—¶é—´åºåˆ—æ•°æ®',
        'neuron_activity_stats.npz': 'ç¥ç»å…ƒæ´»åŠ¨ç»Ÿè®¡æ•°æ®',
        'neuron_count_analysis.npz': 'ç¥ç»å…ƒæ•°é‡åˆ†ææ•°æ®',
        'rr_neurons_distribution.npz': 'RRç¥ç»å…ƒåˆ†å¸ƒæ•°æ®',
        'RR_Neurons_Results.mat': 'RRç¥ç»å…ƒç­›é€‰ç»“æœ',
        'network_analysis_results.npz': 'ç½‘ç»œåˆ†æç»“æœæ•°æ®',
        'centrality_analysis.npz': 'ä¸­å¿ƒæ€§åˆ†ææ•°æ®',
        'rich_club_analysis.npz': 'å¯Œäººä¿±ä¹éƒ¨åˆ†ææ•°æ®',
        'small_world_analysis.npz': 'å°ä¸–ç•Œç½‘ç»œåˆ†ææ•°æ®'
    }
    
    for key, desc in descriptions.items():
        if key in filename:
            return desc
    
    return 'åˆ†ææ•°æ®æ–‡ä»¶'

def generate_analysis_summary(log_file, results_summary):
    """ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š"""
    log_message("\n" + "="*80, log_file)
    log_message("åˆ†æå®Œæˆæ€»ç»“æŠ¥å‘Š", log_file)
    log_message("="*80, log_file)
    
    # æˆåŠŸçš„åˆ†æ
    successful = [item for item, success in results_summary.items() if success]
    failed = [item for item, success in results_summary.items() if not success]
    
    log_message(f"æˆåŠŸå®Œæˆçš„åˆ†æ ({len(successful)}/{len(results_summary)}):", log_file)
    for item in successful:
        log_message(f"  âœ“ {item}", log_file)
    
    if failed:
        log_message(f"\nå¤±è´¥çš„åˆ†æ ({len(failed)}/{len(results_summary)}):", log_file)
        for item in failed:
            log_message(f"  âœ— {item}", log_file)
    
    # ç”Ÿæˆç»“æœç›®å½•ä¿¡æ¯
    log_message("\nç”Ÿæˆçš„ç»“æœç›®å½•:", log_file)
    base_dirs = []
    
    if hasattr(cfg, 'DATA_PATH'):
        base_dirs.append(cfg.DATA_PATH)
    base_dirs.append('results')
    
    for base_dir in base_dirs:
        if os.path.exists(base_dir):
            log_message(f"\n{base_dir}/ ç›®å½•ç»“æ„:", log_file)
            for root, dirs, files in os.walk(base_dir):
                level = root.replace(base_dir, '').count(os.sep)
                indent = '  ' * level
                log_message(f"{indent}{os.path.basename(root)}/", log_file)
                subindent = '  ' * (level + 1)
                for file in files:
                    if file.endswith(('.png', '.npz', '.mat', '.csv')):
                        log_message(f"{subindent}{file}", log_file)
    
    log_message("\n" + "="*80, log_file)

def main():
    """ä¸»å‡½æ•°ï¼šä¸€é”®è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
    print("="*80)
    print("ç¥ç»æ•°æ®å®Œæ•´åˆ†ææµç¨‹")
    print("="*80)
    print(f"æ•°æ®åŠ è½½ç‰ˆæœ¬: {cfg.LOADER_VERSION}")
    print(f"æ•°æ®è·¯å¾„: {getattr(cfg, 'DATA_PATH', 'results')}")
    print("="*80)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = create_analysis_log()
    log_message("å¼€å§‹å®Œæ•´åˆ†ææµç¨‹", log_file)
    log_message(f"æ•°æ®åŠ è½½ç‰ˆæœ¬: {cfg.LOADER_VERSION}", log_file)
    
    # åˆ†ææ¨¡å—é…ç½®
    analysis_modules = [
        ('loaddata', 'åŸºç¡€æ•°æ®å¤„ç†ä¸RRç¥ç»å…ƒç­›é€‰'),
        ('network', 'ç½‘ç»œæ‹“æ‰‘åˆ†æ'),
        ('advanced_network', 'é«˜çº§ç½‘ç»œåˆ†æ'),
        ('noise_correlation', 'å™ªå£°ç›¸å…³æ€§åˆ†æ'),
        ('degree_centrality', 'åº¦ä¸­å¿ƒæ€§ä¸ç¥ç»ä¿¡æ¯å…³ç³»åˆ†æ'),
        ('manifold', 't-SNEæµå½¢å­¦ä¹ ä¸é™ç»´åˆ†æ')
    ]
    
    # æ‰§è¡Œåˆ†æ
    total_start_time = time.time()
    results_summary = {}
    
    for module_name, description in analysis_modules:
        log_message(f"\n{'='*60}", log_file)
        success = run_analysis_module(module_name, description, log_file)
        results_summary[description] = success
        
        # å¦‚æœåŸºç¡€æ¨¡å—å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
        if not success and module_name in ['loaddata']:
            response = input(f"\n{description} å¤±è´¥ï¼Œè¿™å¯èƒ½å½±å“åç»­åˆ†æã€‚æ˜¯å¦ç»§ç»­? (y/N): ")
            if response.lower() != 'y':
                log_message("ç”¨æˆ·é€‰æ‹©åœæ­¢åˆ†æ", log_file)
                break
        
        # æ¨¡å—é—´ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…èµ„æºå†²çª
        time.sleep(2)
    
    # è®¡ç®—æ€»è€—æ—¶
    total_elapsed = time.time() - total_start_time
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_analysis_summary(log_file, results_summary)
    log_message(f"\næ€»åˆ†ææ—¶é—´: {total_elapsed:.1f}ç§’ ({total_elapsed/60:.1f}åˆ†é’Ÿ)", log_file)
    
    # ç”Ÿæˆæ•´åˆçš„MarkdownæŠ¥å‘Š
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆæ•´åˆåˆ†ææŠ¥å‘Š...")
    try:
        report_file = generate_markdown_report(log_file, results_summary, total_elapsed)
        html_file = report_file.replace('.md', '.html')
        print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
        print("ğŸ’¡ å»ºè®®ä½¿ç”¨HTMLç‰ˆæœ¬æŸ¥çœ‹å›¾ç‰‡ï¼Œå›¾ç‰‡æ˜¾ç¤ºæ›´ç¨³å®š")
    except Exception as e:
        print(f"âš ï¸  æ•´åˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    # æœ€ç»ˆæç¤º
    successful_count = sum(results_summary.values())
    total_count = len(results_summary)
    
    print(f"\n{'='*80}")
    print(f"åˆ†ææµç¨‹å®Œæˆï¼")
    print(f"æˆåŠŸ: {successful_count}/{total_count} ä¸ªæ¨¡å—")
    print(f"æ€»è€—æ—¶: {total_elapsed:.1f}ç§’ ({total_elapsed/60:.1f}åˆ†é’Ÿ)")
    print(f"è¯¦ç»†æ—¥å¿—: {log_file}")
    if 'report_file' in locals():
        print(f"MarkdownæŠ¥å‘Š: {report_file}")
        print(f"HTMLæŠ¥å‘Š: {report_file.replace('.md', '.html')} (æ¨èç”¨äºæŸ¥çœ‹å›¾ç‰‡)")
    print(f"{'='*80}")
    
    # å¦‚æœæ‰€æœ‰åˆ†æéƒ½æˆåŠŸï¼Œæ˜¾ç¤ºä¸»è¦ç»“æœä½ç½®
    if successful_count == total_count:
        print("\nğŸ‰ æ‰€æœ‰åˆ†ææ¨¡å—è¿è¡ŒæˆåŠŸ!")
        print("\nğŸ“ ä¸»è¦ç»“æœä½ç½®:")
        print("â”œâ”€â”€ results/figures/ - æ‰€æœ‰å¯è§†åŒ–å›¾åƒ")
        print("â”œâ”€â”€ results/ - æ•°å€¼åˆ†æç»“æœ(.npzæ–‡ä»¶)")
        print("â”œâ”€â”€ [æ•°æ®è·¯å¾„]/analysis_logs/ - è¿è¡Œæ—¥å¿—å’Œæ•´åˆæŠ¥å‘Š")
        if hasattr(cfg, 'DATA_PATH'):
            print(f"â”œâ”€â”€ {cfg.DATA_PATH}/centrality_results/ - ä¸­å¿ƒæ€§åˆ†æç»“æœ")
            print(f"â”œâ”€â”€ {cfg.DATA_PATH}/advanced_analysis/ - é«˜çº§ç½‘ç»œåˆ†æ")
            print(f"â””â”€â”€ {cfg.DATA_PATH}/noise_correlation/ - å™ªå£°ç›¸å…³åˆ†æ")
        
        print(f"\nğŸ“Š æŸ¥çœ‹æ•´åˆæŠ¥å‘Šè·å–å®Œæ•´çš„å¯è§†åŒ–ç»“æœå±•ç¤º!")
    else:
        print(f"\nâš ï¸  {total_count - successful_count} ä¸ªæ¨¡å—è¿è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­åˆ†ææµç¨‹")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nåˆ†ææµç¨‹å‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        sys.exit(1)